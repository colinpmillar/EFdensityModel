\documentclass{article}

\RequirePackage{geometry}
\geometry{verbose,tmargin=35mm,bmargin=30mm,lmargin=35mm,rmargin=35mm}


% ******************************* Line Spacing *********************************

%\renewcommand\baselinestretch{1.2}

% Choose linespacing as appropriate. Default is one-half line spacing as per the
% University guidelines

% \doublespacing
% \onehalfspacing
% \singlespacing
\setlength{\parindent}{0pt}
\setlength{\parskip}{12pt}


% ************************* Algorithms and Pseudocode **************************


\usepackage{algorithmicx}
\usepackage{algpseudocode}

% ***************************** Math and SI Units ******************************

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{siunitx} % use this package module for SI units
\usepackage{bm}           % load after all math to give access to bold math

% Distributions
\DeclareMathOperator{\GammaD}{Gamma}
\DeclareMathOperator{\NormD}{N}
\DeclareMathOperator{\BetaD}{Beta}
\DeclareMathOperator{\tD}{t}
\DeclareMathOperator{\BinomD}{Bin}
\DeclareMathOperator{\PoissonD}{Pois}
\DeclareMathOperator{\NegbinD}{NegBin}

% Functions
\DeclareMathOperator{\Beta}{B}

% Operators
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\CV}{CV}

% likelihood functions
\newcommand{\prob}[1]{\mathrm{Pr}\left( #1 \right)}
\newcommand{\Lik}[1]{\mathrm{L}\left( #1 \right)}
\newcommand{\logLik}[1]{l\left( #1 \right)}




% ******************************* Meta data ************************************

\title{Statistical aspects of Electrofishing}
\author{Colin Millar}
\date{\Sexpr{Sys.Date()}}

\begin{document}

<<settings, echo=FALSE, message=FALSE>>=
library(ef, quietly = TRUE)
library(dplyr, quietly = TRUE)
library(tidyr, quietly = TRUE)
library(mgcv, quietly = TRUE)
@

\maketitle

\tableofcontents

Note that this is being drafted in paper style for my own purposes.  We can probably just thin this down a bit and turn this into a methods section and perhaps put other bits, if nessissary, into a short appendix.  It is also still developing so dont consider this as a draft as yet.  The methods are still under development.






% ------------------------------------------------------------------------------
%
%
%
% ------------------------------------------------------------------------------

\section{Introduction}

Large multispecies age-structured electrofishing datasets are statistically complex to model because of heirarchical structure in the data. Two processes define an electrofishing sample: capture probability $p$ and fish abundance $N$.  Millar et al. (2015) presented a model where capture probability was shown to vary with covariates describing habitat and sampling procedure.  After controling for these factors Millar et al. found that there remained considerable variation in the observations at the sample level. This between sample variation could be due to a number of variables not included in the model such as water temperature at time of sampling, electrical current used, light attenuation / cloud cover etc.  This additional variability was accounted for by using an approximate quasi-likelihood approach.

  In this document the same variables used in Millar et al. are used to model fish abundance and as with capture probability it is likely that fish abundance will show additional variability due to factors not included in the model, such as the proximity of redds or fine-scale habitat features. Additionally, it is also possible that the presence of one lifestage or species could inhibit or encourage the presence of another. Because of this, the abundances of each fish-type may show correlation between samples.

  This document develops a modelling approach to allow efficient model selection while accounting for between sample variation accross fish-type using generalised additive models.  The sampling efficiency of each sample is derived from a capture probability model arrived at using the procedure set out in Millar et al. (2015). This provides an offset with which to model density from the total catch. In order to correctly assess the significance of terms in the density model, it is, therefore,  nessisary to account for the uncertainty due to the estimation of capture probability. This is achieved using a simulation procedure.

  This document procedes as follows. First a model for single sample is described and then the case where a number of samples are made over sites with the same characteristics is considered.  This allows the development of an approach for dealing with correlation between lifestages.  This model is then extended to allow for habitat and temporal variation.









% ------------------------------------------------------------------------------
%
%
%
% ------------------------------------------------------------------------------

\section{A density model for a single sample}


The density $\lambda_k$ of fish-type $k$ ($k =$ trout-fry, trout-parr, salmon-fry, salmon-parr) at a single electrofishing site is determined solely by fish type.  Density is often modelled on the log scale so that

\begin{align}
  \log \lambda_k = \nu_k
\end{align}

If it is assumed that the abundance $N_k$ of each fish-type in the fished area at the time of sampling is a realisation of a Poisson process with rate $\lambda_k$, then the total count $T_k$ of each fish type is Poisson distributed with rate $q_kA\lambda_k$, where $q_k$ is the average probability of a fish of type $k$ apearing in the sample (Huggins, 1996) and $A$ is the area fished.  The parameters $\nu_k$ can then be estimated by maximising the Poisson log-likelihood

\begin{align}
  \log L = \sum_k T_k\nu_k - \exp\big(\nu_k + \log q_k + \log A \big)
\end{align}


Estimating $\nu_k$ is straightforward if $q_k$ is assumed known without error by using standard software for generalised linear models (GLMs).  For convience let
\begin{align}
  \bm{\nu} =
    \begin{pmatrix}
      \nu_{\text{trout-fry}} \\ \nu_{\text{trout-parr}} \\
      \nu_{\text{salmon-fry}} \\ \nu_{\text{salmon-parr}}
    \end{pmatrix}
\intertext{and}
  \bm{q} =
    \begin{pmatrix}
      q_{\text{trout-fry}} \\ q_{\text{trout-parr}} \\
      q_{\text{salmon-fry}} \\ q_{\text{salmon-parr}}
    \end{pmatrix}
\end{align}

then, if there is sufficient numers of fish caught, the maximum likelihood estimate $\hat{\bm{\nu}}|\hat{\bm{q}}$ (i.e. conditional on the estimate of $q_k$ from the capture probability model) is multivariate normally distributed with mean $\bm{\nu}|\hat{\bm{q}}$ and with a diagonal covariance matrix. Diagonal beacause conditional on $q_k$, the total counts are realisations of independent poisson distributions.

However, because $\hat{\bm{q}}$ is itself an estimate, it also has a distribution and the covariance matrix of which may not be diagonal.  Hence, after incorporating the error from the capture probability model into the density model, the unconditional estimate $\hat{\bm{\nu}}$ is approximately
\begin{align}
  \hat{\bm{\nu}} \sim \NormD(\bm{\nu}, \bm{\Sigma})
\end{align}
where $\bm{\Sigma}$ is approximately
\begin{align}
  \label{eq:hugginserror}
  I(\hat{\bm{\nu}}|\hat{\bm{q}})^{-1} + \frac{d\hat{\bm{q}}}{d \beta}' I(\hat{\beta})^{-1} \frac{d\hat{\bm{q}}}{d \beta}
\end{align}
where $I$ denotes the fisher information matrix and $\beta$ are the model parameters involved in estimating capture probability (see Huggins for more detail).  This variance matrix can be used to construct approximate pointwise confidence intervals or conduct hypothesis tests.

An alternative approach, and likely more accurate, way to derive the distribution of $\hat{\bm{\nu}}$ is through bootstrapping.  This would be achieved as follows

\begin{enumerate}
  \item Take a bootstrap resample, $\bm{y}^*$ of the data $\bm{y}$, where a sample is an electrofishing site visit
  \item Find the MLEs $\bm{\hat{\beta}}^*$ of the parameters of the selected capture probabilty model
  \item Calculate the offset $\bm{\hat{q}}^*$
  \item Find the MLEs $\bm{\hat{\nu}}^*|\bm{\hat{q}}^*$ given the same datset $\bm{y}^*$.
  \item Store the estimates $\bm{\hat{\nu}}^*$ and intermediate steps if desired.
  \item Repeat lots of times
\end{enumerate}

Calculating the variance of the realisations $\bm{\hat{\nu}}^*$ will result in an estimate of $\bm{\Sigma}$.



% ------------------------------------------------------------------------------
%
%
%
% ------------------------------------------------------------------------------

\section{Between sample variation}

Following Fryer (1991) between sample variation can be incorporated using a two stage procedure where
\begin{align}
  \bm{\nu}_i \sim \NormD(\bm{\alpha}, \bm{\Sigma}_i + \bm{D})
\end{align}

Here, the variance can be decomposed into an estimation error for each sample (site visit) $i$, $\bm{\Sigma}_i$, which is a combination of the density model parameter estimation error and the capture probability model parameter estimation error, and a second term $\bm{D}$ which describes the between sample covariance. The variance $\bm{\Sigma}_i$ can be estimated as described above, while the estimate of $\bm{D}$ is calculated following Fryer (1991).


Given the covariance $\bm{D}$, the parameters $\bm{\alpha}$ can be estimated by maximising the penalised log-likelihood
\begin{align}
  \log L_p &= \log L - \text{penalty} \nonumber \\
  \log L_p &= \sum_{ik} T_{ik} (\alpha_k + \eta_{ik}) - \exp\big(\alpha_k + \eta_{ik} + \log q_{ik} + \log A_i \big) -  \frac{1}{2}\sum_i \bm{\eta}_i' \bm{D^{-1}\eta}_i
\end{align}
where
\begin{align}
  \bm{\eta_{i}} \sim \NormD(\bm{0}, \bm{D})
\end{align}
this model can be fitted in mgcv by specifying a penalty matrix common to each sample, which is equal to $\frac{1}{2}\bm{D}^{-1}$.  This is done in practive using the texttt{paraPen} argument, see \texttt{?gam.models} for more information.

The parameter estimates arising from the above penalised likelihood are conditional on the estimate of capture probability $\hat{\bm{q}}$, and so to derive appropriate confidence intervals and hypothesis tests, the estimation error from the capture probability model must be propagated.  This can be done either by the approach used in Huggins (equation \ref{eq:hugginserror}), or by the alternative bootstrap procedure described in the previous section, or a combination thereof.

The following section gives an example how known between sample covariance $\bm{D}$ can be incorporated in a GAM.






% ------------------------------------------------------------------------------
%
%
%
% ------------------------------------------------------------------------------

\section{Quick go at estimating D}


<<quick_try>>=

# simulate a correlated process
Di <- matrix(c(1, -.8, -.8, 1), 2, 2)
nsamples <- 100
lambda <- MASS::mvrnorm(n = nsamples, mu = c(1,4), Sigma = Di)
obs_lambda <- lambda + rnorm(nsamples*2, 0, sqrt(2))

dat <- data.frame(stage = factor(rep(c("fry", "parr"), each = nsamples)),
                  id = factor(1:(2*nsamples)),
                  sample = factor(rep(1:nsamples, 2)),
                  density = c(obs_lambda))

# set up smoother for fitting
# this is the inverse of the joint covariance matrix
D <- solve(Di %x% diag(nsamples))

# design matrices
Xm <- model.matrix(~ stage - 1, data = dat)
Xi <- model.matrix(~ id - 1, data = dat)

# now lets fit in mgcv
library(mgcv)
# first double the length of dat and then weight to zero
#  to fool gam into thinking there is enough data
dat2 <- rbind(dat, dat)
dat2 $ weights <- rep(1:0, each = nrow(dat))
dat2 $ Xm <- rbind(Xm, Xm)
dat2 $ Xi <- rbind(Xi, Xi)

# fit
g1 <- gam(density ~ Xm + Xi - 1, paraPen=list(Xi=list(0.5*D, sp = 1)),
          data = dat2, weights = weights)
# scale should be 0.2
g1 $ sig2
# no. of parameters estimated = 202
# (no. of samples * 2 stages + 2 stage means)
# but number of model degrees of freedom:
sum(g1$edf)
# fitted sample effects
dat $ fitted <- Xm %*% coef(g1)[grep("Xm", names(coef(g1)))] +
                Xi %*% coef(g1)[grep("Xi", names(coef(g1)))]
#
fitted <- matrix(dat$fitted, ncol = 2, dimnames = list(NULL, c("fry", "parr")))
par(mfrow = c(2,2))
plot(obs_lambda)
plot(fitted)
plot(lambda, col = "red", cex = 0.5)
points(fitted, cex = 0.5)
plot(lambda, type = "n")
arrows(fitted[,1], fitted[,2], lambda[,1], lambda[,2], length = 0.05, code = 1)
cor(fitted)
cov(fitted)
cov(lambda)

# have a look at confidence intervals around stage effect
cbind(est = coef(g1)[1:2],
      sd = sqrt(diag(vcov(g1)[1:2,1:2])))
cov2cor(vcov(g1)[1:2,1:2])

# what if we didnt model the correlation...
g2 <- lm(density ~ stage - 1, data = dat)
cbind(est = coef(g2),
      sd = sqrt(diag(vcov(g2))))
cov2cor(vcov(g2))
@








% ------------------------------------------------------------------------------
%
%
%
% ------------------------------------------------------------------------------

\section{Quick go at estimating D version 2}


<<quick_try2>>=
## get previous data...
dat <- data.frame(y0 = obs_lambda[,1], y1 = obs_lambda[,2])

## fit model...

b <- gam(list(y0 ~ 1, y1 ~ 1), family=mvn(d=2), data=dat)
b
summary(b)
solve(crossprod(b$family$data$R)) ## estimated cov matrix
# compare to
cov(fitted) + diag(g1$sig2, 2)
@






% ------------------------------------------------------------------------------
%
%
%
% ------------------------------------------------------------------------------

\section{Cursory look at between sample variation in density}

Below is a plot of estimated Fry against estimated Parr abundance within the same site.  The abudnaces were estimated by
\begin{align}
  \hat{N}_{ik} = \frac{T_{ik}}{\hat{q}_{ik}}
\end{align}


<<get_captureprob_paper_densities, echo = FALSE, cache = TRUE, message = FALSE>>=
# install release version 1.0 of ef
#httr::set_config(httr::use_proxy(url="192.168.41.8", port=80))
#devtools::install_github("faskally/ef@v1.0")

# load appropriate data
load("C:/work/repos/papers/capture_prop_paper/intermediate_rData/phi.rData") # phi
load("C:/work/repos/papers/capture_prop_paper/intermediate_rData/phi_new.rData") # phi
load("C:/work/repos/papers/capture_prop_paper/intermediate_rData/screenedData.rData") # ef

# calculate nicer covariates
ef <- within(ef, {
              cDistance_s = c(scale(Distance_s))
              cWater_W = c(scale(Water_W))
              cElevation_ = c(scale(Elevation_))
              fyear = factor(fyear)
              Trust = factor(Trust)
              LifeStage = factor(LifeStage)
              })
contrasts(ef $ fyear) <- "contr.sum"
contrasts(ef $ Trust) <- "contr.treatment"

# fit model
modelledpf <-  n ~ LifeStage + Trust + fyear + pass23 + cWater_W +
                   cElevation_ + cDistance_s +
                   LifeStage:pass23 + s(doy, k = 3, by = LifeStage) +
                   cElevation_:LifeStage
modelledp <- efp(modelledpf, data = ef, pass = pass)

# get density estimates
ef $ pijk <- modelledp $ fitted

# by sample/species/lifetage
ef2 <- ef %>%
       select(sampleID, LifeStage, pass, pijk) %>%
       as.data.frame(.) %>%
       spread(pass, pijk)

ef2 $ p <- 1 - (1-ef2[["1"]]) * (1-ef2[["2"]]) * (1-ef2[["3"]])
ef2 $ n <- c(tapply(ef $ n, list(ef $ LifeStage, ef $ sampleID), sum))
ef2 $ abundance <- ef2$n / ef2 $ p

abundance <- t(matrix(ef2 $ abundance, nrow = 2))
plot(log(abundance), axes = FALSE, xlab = "fry", ylab = "parr", main = "Fry vs Parr abundance")
axis(1, at = 0:6, labels = round(exp(0:6), 1))
axis(2, at = 0:6, labels = round(exp(0:6), 1))
@




% ------------------------------------------------------------------------------
%
%
%
% ------------------------------------------------------------------------------

%\section{An analysis of simulated data}

<<simulation_settings, echo=FALSE>>=
n <- 30
rs <- 0.5; rl <- -0.8
sig <- 0.1
alpha <- rep(log(40), 4)
#
Rl <- diag(2); Rl[2:3] <- rl
Rs <- diag(2); Rs[2:3] <- rs
R <- Rs %x% Rl
D <- sig*R
@

%Simulation scheme is as follows. \Sexpr{n} electrofishing samples are simulated, in order to make things interesting it is assmumed that salmon and trout are often found together while parr inhibit fry, so that
%\begin{align}
%  \bm{D} = \sigma^2 \bm{R} = \sigma^2 \big( \bm{R}_\text{species} \otimes \bm{R}_\text{life-stage} \big)
%\end{align}
%where the $\bm{R}$ matrixes are 2x2 correlation matrices with a correlation of \Sexpr{rs} between species and \Sexpr{rl} between lifestages resulting in
%\begin{align}
%  \bm{R} =
%    \begin{bmatrix}
%      1.0 & -0.8 &  0.5 & -0.4 \\
%     -0.8 &  1.0 & -0.4 &  0.5 \\
%      0.5 & -0.4 &  1.0 & -0.8 \\
%     -0.4 &  0.5 & -0.8 &  1.0
%    \end{bmatrix}
%\end{align}
%Simulated densities for each fish type are shown below, each has a mean of 40 fish per metre square and a CV of around 10\%

<<simulation_nu, echo=FALSE>>=
set.seed(235987)
numat <- MASS::mvrnorm(n, alpha, D)
colnames(numat) <- c("Trout-fry", "Trout-parr", "Salmon-fry", "Salmon-parr")
ndata <- expand.grid(lifestage = c("fry", "parr"),
                     species = c("Trout", "Salmon"),
                     sample = 1:n)
ndata $ nu <- exp(c(numat))
@

<<simulation_nu_plot, echo=FALSE, eval = FALSE>>=
plot(as.data.frame(exp(numat)))
@

<<simulation_p, echo=FALSE>>=
pdata <- expand.grid(pass = 1:3,
                     lifestage = c("fry", "parr"),
                     species = c("Trout", "Salmon"),
                     sample = 1:n)
pdata$id <- with(pdata, as.numeric(factor(paste(sample, lifestage, species))))
pmodel <- ~ lifestage + species
Xp <- model.matrix(pmodel, pdata)
betap <- c(0.5, 0.5, -0.2)
pdata $ p <- 1/(1+exp(-Xp %*% betap))
pmat <- matrix(pdata $ p, 3)
pimat <- rbind(pmat[1,], (1-pmat[1,])*pmat[2,], (1-pmat[1,])*(1-pmat[1,])*pmat[3,])
pdata $ pi <- c(pimat)
pdata $ row <- 1:nrow(pdata)
pdata <- merge(pdata, ndata)
pdata <- pdata[order(pdata$row),]
@

<<simulation_y, echo=FALSE>>=
# y is poisson(pi * A * nu)
set.seed(345455)
pdata $ y <- rpois(nrow(pdata), pdata$pi * pdata$nu)
ndata $ y <- colSums(matrix(pdata $ y, 3))
@

%having simulated some data we can fit a capture probability model

<<fitting_p, message=FALSE, eval = FALSE, echo=FALSE>>=
pmod <- efp(y ~ lifestage + species, pass = pass, id = id,
            data = pdata)
@

%And now we can fit a density model to each sample


<<fitting_nu, eval = FALSE, echo=FALSE>>=
pmat <- matrix(fitted(pmod, type = "p"), 3)
ndata $ pi <- 1 - (1-pmat[1,])*(1-pmat[1,])*(1-pmat[3,])
nmod <- glm(y ~ lifestage:species:factor(sample)-1, family = poisson,
            data = ndata, offset = log(ndata$pi))
@

%Now we can calculate $R_i$ for each $\nu_i$. $R_i$ is the total error in estimating $\nu_i$, one way to get this is via bootstrapping

<<boot_nu, cache=TRUE, message=FALSE, eval = FALSE, echo=FALSE>>=
do.one.boot <- function(...) {
  bsamp <- sample(unique(ndata$sample),
                  length(unique(ndata$sample)),
                  replace = TRUE)
  bpdata <- pdata[pdata$sample %in% bsamp,]
  bndata <- ndata[ndata$sample %in% bsamp,]

  pmod <- efp(y ~ lifestage + species, pass = pass, id = id,
              data = bpdata)
  pmat <- matrix(fitted(pmod), 3)
  bndata $ pi <- 1 - (1-pmat[1,])*(1-pmat[1,])*(1-pmat[3,])
  nmod <- glm(y ~ lifestage:species:factor(sample)-1, family = poisson,
              data = bndata, offset = log(bndata$pi))
  out <- matrix(coef(nmod), nrow = 4)
  colnames(out) <- paste(sort(unique(bsamp)))
  # add in poisson estimation error
  out[] <- MASS::mvrnorm(1, coef(nmod), vcov(nmod))
  out
}
nboot <- 1000
sim <- replicate(nboot, do.one.boot(), simplify = FALSE)
sim <- sapply(sim,
              function(x) {
                out <- matrix(NA, 4, n)
                colnames(out) <- 1:n
                out[,colnames(x)] <- x
                out
              })
dim(sim) <- c(4, n, nboot)
Ri <- lapply(1:n,
            function(i) {
              x <- t(sim[,i,])
              var(x, na.rm = TRUE)
            })
Ri[[1]]
unname(vcov(nmod)[1:4,1:4])
@


%another way to get this is via MCMC sampling

<<mcmc_nu, cache=TRUE, message=FALSE, eval = FALSE, echo=FALSE>>=
samp <- simulate(pmod, nsim = 1000)
library(rstan)
sim <- extract(samp)$piT

dim(sim) <- c(4, n, 1000)
Ri <- lapply(1:n,
            function(i) {
              x <- t(sim[,i,])
              var(x, na.rm = TRUE)
            })
Ri[[1]]
unname(vcov(nmod)[1:4,1:4])
@


%And then estimate $D$!


%Then finally re-estmimate nu and get appropriate confidence intervals!










% ------------------------------------------------------------------------------
%
%
%
% ------------------------------------------------------------------------------

%\section{Modelling abundance accross Scotland}

%Here we extend the model to incorporate covariates.  So the task here is to fit suitable complex models to each species-lifestage, then fit a sample-wise model with the predictions from the complex models as offsets. THen estimate $D$ from these.




\end{document}
